#########################生产者##########################
producer.sources = source1
producer.channels = channel1
producer.sinks = sink1

#无syslogtcp 测试环境,netcat 暂时替代
producer.sources.source1.channels = channel1
producer.sources.source1.type= netcat
producer.sources.source1.bind= 0.0.0.0
producer.sources.source1.port= 44444

#syslog 丢数据,rsyslog 不丢数据
#producer.sources.source1.type = syslogtcp
#producer.sources.source1.port = 5140
#producer.sources.source1.host = 0.0.0.0
#producer.sources.source1.channels = c

producer.sources.source1.interceptors = i1 i2 i3
producer.sources.source1.interceptors.i1.type = regex_extractor
producer.sources.source1.interceptors.i1.regex = (channel\\d+)
producer.sources.source1.interceptors.i1.serializers = s1
producer.sources.source1.interceptors.i1.serializers.s1.name = key

#producer.sources.source1.interceptors.i2.type = timestamp
producer.sources.source1.interceptors.i2.type = regex_extractor
producer.sources.source1.interceptors.i2.regex = ^(?:\\n)?(\\d\\d\\d\\d/\\d\\d/\\d\\d\\s\\d\\d:\\d\\d)
#producer.sources.source1.interceptors.i2.regex = ^(?:\\n)?(\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d)
producer.sources.source1.interceptors.i2.serializers = s1
producer.sources.source1.interceptors.i2.serializers.s1.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer
producer.sources.source1.interceptors.i2.serializers.s1.name = timestamp
producer.sources.source1.interceptors.i2.serializers.s1.pattern = yyyy/MM/dd HH:mm
#producer.sources.source1.interceptors.i2.serializers.s1.pattern = yyyy-MM-dd HH:mm

producer.sources.source1.interceptors.i3.type = regex_extractor
producer.sources.source1.interceptors.i3.regex = (\\d+\\.\\d+\\.\\d+\\.\\d+)
producer.sources.source1.interceptors.i3.serializers = s1
producer.sources.source1.interceptors.i3.serializers.s1.name = ip


#producer.sinks.sink1.type = logger
#producer.sinks.sink1.channel = channel1

##老版本插件支持
#producer.sinks.sink1.type = org.apache.flume.plugins.KafkaSink
##kafka interface
##docker link names
#producer.sinks.sink1.metadata.broker.list=kafkproducer:9092
#producer.sinks.sink1.serializer.class=kafka.serializer.StringEncoder
#producer.sinks.sink1.request.required.acks=1
#producer.sinks.sink1.max.message.size=1000000
#producer.sinks.sink1.custom.topic.name=mykafka
#producer.sinks.sink1.channel = channel1

#新版本1.6原生支持
#kafka 数据生产者
producer.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink
producer.sinks.sink1.topic = mykafka
producer.sinks.sink1.brokerList = kafka1:9092,kafka2:9092,kafka3:9092
producer.sinks.sink1.requiredAcks = 1
producer.sinks.sink1.batchSize = 20
producer.sinks.sink1.channel = channel1



producer.channels.channel1.type = memory
producer.channels.channel1.capacity = 1000

#测试ok ,为方便测试,展示更换为memory
#echo "hello,jamesmo" | nc 192.168.59.103 5140
#producer.channels.channel1.type   = org.apache.flume.channel.kafka.KafkaChannel
#producer.channels.channel1.capacity = 10000
#producer.channels.channel1.transactionCapacity = 1000
#producer.channels.channel1.brokerList=kafka1:9092,kafka2:9092,kafka3:9092
#producer.channels.channel1.topic=channel1
#producer.channels.channel1.zookeeperConnect=zk:2181

#######################消费者#############################

consumer.sources = source2
consumer.channels = channel2
consumer.sinks = sink2

#kafka 数据消费者,flume 数据源
consumer.sources.source2.type = org.apache.flume.source.kafka.KafkaSource
consumer.sources.source2.channels = channel2
consumer.sources.source2.zookeeperConnect = 192.168.59.103:2181
consumer.sources.source2.topic = mykafka
consumer.sources.source2.groupId = flume
consumer.sources.source2.kafka.consumer.timeout.ms = 100

consumer.sources.source2.interceptors = i1 i2 i3
consumer.sources.source2.interceptors.i1.type = regex_extractor
consumer.sources.source2.interceptors.i1.regex = (channel\\d+)
consumer.sources.source2.interceptors.i1.serializers = s1
consumer.sources.source2.interceptors.i1.serializers.s1.name = key

#consumer.sources.source2.interceptors.i2.type = timestamp
consumer.sources.source2.interceptors.i2.type = regex_extractor
consumer.sources.source2.interceptors.i2.regex = ^(?:\\n)?(\\d\\d\\d\\d/\\d\\d/\\d\\d\\s\\d\\d:\\d\\d)
#consumer.sources.source2.interceptors.i2.regex = ^(?:\\n)?(\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d)
consumer.sources.source2.interceptors.i2.serializers = s1
consumer.sources.source2.interceptors.i2.serializers.s1.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer
consumer.sources.source2.interceptors.i2.serializers.s1.name = timestamp
consumer.sources.source2.interceptors.i2.serializers.s1.pattern = yyyy/MM/dd HH:mm
#consumer.sources.source2.interceptors.i2.serializers.s1.pattern = yyyy-MM-dd HH:mm

consumer.sources.source2.interceptors.i3.type = regex_extractor
consumer.sources.source2.interceptors.i3.regex = (\\d+\\.\\d+\\.\\d+\\.\\d+)
consumer.sources.source2.interceptors.i3.serializers = s1
consumer.sources.source2.interceptors.i3.serializers.s1.name = ip

#数据消费输出,hdbs/hbase/hive其他渠道可以单独测试,正式环境进行替换即可
#consumer.sinks.sink2.type = logger
#consumer.sinks.sink2.channel = channel2

consumer.sinks.sink2.type = file_roll
consumer.sinks.sink2.channel = channel2
consumer.sinks.sink2.sink.directory = /var/log/flume

consumer.channels.channel2.type = memory
consumer.channels.channel2.capacity = 1000
#kafka-topics.sh --create --zookeeper 192.168.59.103:2181 --replication-factor 1 --partitions 1 --topic mykafka
#bin/flume-ng agent --conf conf --conf-file conf/flume.conf.mykafka --name producer -Dflume.root.logger=INFO,console
